{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344848e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 reading data and covert to df\n",
    "from scipy.io import wavfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyreadr\n",
    "\n",
    "# big to do: read wave values by each eyemovement from wave file, not whole file in dataframe!!!!!!!!\n",
    "\n",
    "# def data pre process function, \n",
    "#wave_seq1[\"id\"] = wave_seq1[\"id\"].astype('float')\n",
    "#wave_seq1[\"values\"] = wave_seq1[\"values\"].astype('float')\n",
    "#return a cleaned wave dataframe\n",
    "\n",
    "#get our dataset: wave_seq(a dataframe combining all eyemovemnt, which is our dataset)\n",
    "# df col:wave value, id = index of eye movement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4b139f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       values   id\n",
      "0        21.0  1.0\n",
      "1        19.0  1.0\n",
      "2        18.0  1.0\n",
      "3        17.0  1.0\n",
      "4        16.0  1.0\n",
      "...       ...  ...\n",
      "64401   -48.0  4.0\n",
      "64402   -49.0  4.0\n",
      "64403   -51.0  4.0\n",
      "64404   -51.0  4.0\n",
      "64405   -53.0  4.0\n",
      "\n",
      "[64406 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "wave_seq1 = pd.read_csv(\"first.csv\")\n",
    "\n",
    "wave_seq1[\"id\"]  = \"1\"\n",
    "\n",
    "wave_seq1 = wave_seq1.drop(columns = \"Unnamed: 0\")\n",
    "wave_seq1.rename(columns = {'x':'values'}, inplace = True)\n",
    "\n",
    "wave_seq1[\"id\"] = wave_seq1[\"id\"].astype('float')\n",
    "wave_seq1[\"values\"] = wave_seq1[\"values\"].astype('float')\n",
    "\n",
    "\n",
    "wave_seq2 = pd.read_csv(\"first_1.csv\")\n",
    "\n",
    "wave_seq2[\"id\"]  = \"2\"\n",
    "\n",
    "wave_seq2 = wave_seq2.drop(columns = \"Unnamed: 0\")\n",
    "wave_seq2.rename(columns = {'x':'values'}, inplace = True)\n",
    "\n",
    "wave_seq2[\"id\"] = wave_seq2[\"id\"].astype('float')\n",
    "wave_seq2[\"values\"] = wave_seq2[\"values\"].astype('float')\n",
    "\n",
    "\n",
    "wave_seq3 = pd.read_csv(\"r1.csv\")\n",
    "\n",
    "wave_seq3[\"id\"]  = \"3\"\n",
    "\n",
    "wave_seq3 = wave_seq3.drop(columns = \"Unnamed: 0\")\n",
    "wave_seq3.rename(columns = {'x':'values'}, inplace = True)\n",
    "\n",
    "wave_seq3[\"id\"] = wave_seq3[\"id\"].astype('float')\n",
    "wave_seq3[\"values\"] = wave_seq3[\"values\"].astype('float')\n",
    "\n",
    "\n",
    "wave_seq4 = pd.read_csv(\"r2.csv\")\n",
    "\n",
    "wave_seq4[\"id\"]  = \"4\"\n",
    "\n",
    "wave_seq4 = wave_seq4.drop(columns = \"Unnamed: 0\")\n",
    "wave_seq4.rename(columns = {'x':'values'}, inplace = True)\n",
    "\n",
    "wave_seq4[\"id\"] = wave_seq4[\"id\"].astype('float')\n",
    "wave_seq4[\"values\"] = wave_seq4[\"values\"].astype('float')\n",
    "\n",
    "wave_seq = pd.concat([wave_seq1,wave_seq2,wave_seq3,wave_seq4],ignore_index=True)\n",
    "\n",
    "# downsampling\n",
    "\n",
    "\n",
    "print(wave_seq)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4493bb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling\n",
    "\n",
    "def downsample_dataframe(dataframe, new_sample_rate):\n",
    "    downsampling_factor = int(dataframe.shape[0] / new_sample_rate)\n",
    "    downsampled_dataframe = dataframe.iloc[::downsampling_factor, :]\n",
    "    return downsampled_dataframe\n",
    "\n",
    "\n",
    "def downsample_process(inputdf, ds_df):\n",
    "    if inputdf.shape[0] - ds_df.shape[0] != 0:\n",
    "        return print(\"downsample complete\")\n",
    "    else:\n",
    "        return print(\"downsample not work\")\n",
    "    \n",
    "downsampled_df = downsample_dataframe(wave_seq, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "477fc822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|█████████████████████████| 4/4 [00:00<00:00,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     values__variance_larger_than_standard_deviation  \\\n",
      "1.0                                              1.0   \n",
      "2.0                                              1.0   \n",
      "3.0                                              1.0   \n",
      "4.0                                              1.0   \n",
      "\n",
      "     values__has_duplicate_max  values__has_duplicate_min  \\\n",
      "1.0                        0.0                        0.0   \n",
      "2.0                        0.0                        0.0   \n",
      "3.0                        0.0                        0.0   \n",
      "4.0                        0.0                        0.0   \n",
      "\n",
      "     values__has_duplicate  values__sum_values  values__abs_energy  \\\n",
      "1.0                    1.0            -14601.0          94573425.0   \n",
      "2.0                    1.0              8256.0         155722516.0   \n",
      "3.0                    1.0             -8953.0         410779937.0   \n",
      "4.0                    1.0            -12534.0         383789288.0   \n",
      "\n",
      "     values__mean_abs_change  values__mean_change  \\\n",
      "1.0               135.439490             1.375796   \n",
      "2.0               140.857843             0.622549   \n",
      "3.0               119.564516             0.680645   \n",
      "4.0               113.740964            -0.054217   \n",
      "\n",
      "     values__mean_second_derivative_central  values__median  ...  \\\n",
      "1.0                                1.294872          -136.5  ...   \n",
      "2.0                                0.677340            20.0  ...   \n",
      "3.0                                0.050162            52.0  ...   \n",
      "4.0                               -0.303625           -18.0  ...   \n",
      "\n",
      "     values__fourier_entropy__bins_3  values__fourier_entropy__bins_5  \\\n",
      "1.0                         0.226632                         0.293184   \n",
      "2.0                         0.191142                         0.245414   \n",
      "3.0                         0.125256                         0.215617   \n",
      "4.0                         0.125256                         0.170467   \n",
      "\n",
      "     values__fourier_entropy__bins_10  values__fourier_entropy__bins_100  \\\n",
      "1.0                          0.432325                           0.825939   \n",
      "2.0                          0.389094                           0.689397   \n",
      "3.0                          0.305728                           0.529885   \n",
      "4.0                          0.249958                           0.406332   \n",
      "\n",
      "     values__permutation_entropy__dimension_3__tau_1  \\\n",
      "1.0                                         1.755881   \n",
      "2.0                                         1.743985   \n",
      "3.0                                         1.735973   \n",
      "4.0                                         1.750774   \n",
      "\n",
      "     values__permutation_entropy__dimension_4__tau_1  \\\n",
      "1.0                                         2.968930   \n",
      "2.0                                         3.020781   \n",
      "3.0                                         3.014923   \n",
      "4.0                                         3.028454   \n",
      "\n",
      "     values__permutation_entropy__dimension_5__tau_1  \\\n",
      "1.0                                         3.712694   \n",
      "2.0                                         3.838683   \n",
      "3.0                                         3.842457   \n",
      "4.0                                         3.877711   \n",
      "\n",
      "     values__permutation_entropy__dimension_6__tau_1  \\\n",
      "1.0                                         4.230830   \n",
      "2.0                                         4.446683   \n",
      "3.0                                         4.512036   \n",
      "4.0                                         4.532687   \n",
      "\n",
      "     values__permutation_entropy__dimension_7__tau_1  \\\n",
      "1.0                                         4.563331   \n",
      "2.0                                         4.805552   \n",
      "3.0                                         4.949423   \n",
      "4.0                                         4.983521   \n",
      "\n",
      "     values__mean_n_absolute_max__number_of_maxima_7  \n",
      "1.0                                      2175.428571  \n",
      "2.0                                      2329.285714  \n",
      "3.0                                      3199.857143  \n",
      "4.0                                      2849.714286  \n",
      "\n",
      "[4 rows x 702 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#2. feature extraction\n",
    "\n",
    "#pip install tsfresh\n",
    "from tsfresh import extract_features \n",
    "from tsfresh import extract_relevant_features\n",
    "from tsfresh.feature_extraction import extract_features, EfficientFCParameters\n",
    "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
    "from tsfresh.feature_extraction import MinimalFCParameters, settings\n",
    "\n",
    "import tsfresh\n",
    "\n",
    "ext_feature_df = extract_features(downsampled_df,column_id=\"id\",\n",
    "                                  default_fc_parameters=ComprehensiveFCParameters(),\n",
    "                                  n_jobs=0)\n",
    "\n",
    "ext_feature_clean = ext_feature_df.dropna(axis = 1, how = \"any\")\n",
    "print(ext_feature_clean)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "dfcb8cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nn/h99dr9455yg83gkxbsycq7m40000gn/T/ipykernel_50136/3051204813.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ext_feature_clean[cols] = minmax_scale(ext_feature_clean[cols])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>values__variance_larger_than_standard_deviation</th>\n",
       "      <th>values__has_duplicate_max</th>\n",
       "      <th>values__has_duplicate_min</th>\n",
       "      <th>values__has_duplicate</th>\n",
       "      <th>values__sum_values</th>\n",
       "      <th>values__abs_energy</th>\n",
       "      <th>values__mean_abs_change</th>\n",
       "      <th>values__mean_change</th>\n",
       "      <th>values__mean_second_derivative_central</th>\n",
       "      <th>values__median</th>\n",
       "      <th>...</th>\n",
       "      <th>values__fourier_entropy__bins_3</th>\n",
       "      <th>values__fourier_entropy__bins_5</th>\n",
       "      <th>values__fourier_entropy__bins_10</th>\n",
       "      <th>values__fourier_entropy__bins_100</th>\n",
       "      <th>values__permutation_entropy__dimension_3__tau_1</th>\n",
       "      <th>values__permutation_entropy__dimension_4__tau_1</th>\n",
       "      <th>values__permutation_entropy__dimension_5__tau_1</th>\n",
       "      <th>values__permutation_entropy__dimension_6__tau_1</th>\n",
       "      <th>values__permutation_entropy__dimension_7__tau_1</th>\n",
       "      <th>values__mean_n_absolute_max__number_of_maxima_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800185</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.193383</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.473259</td>\n",
       "      <td>0.613680</td>\n",
       "      <td>0.830239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.649919</td>\n",
       "      <td>0.610735</td>\n",
       "      <td>0.762944</td>\n",
       "      <td>0.674596</td>\n",
       "      <td>0.402455</td>\n",
       "      <td>0.871087</td>\n",
       "      <td>0.763492</td>\n",
       "      <td>0.715084</td>\n",
       "      <td>0.576456</td>\n",
       "      <td>0.150188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247102</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.214757</td>\n",
       "      <td>0.513885</td>\n",
       "      <td>0.221325</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.367919</td>\n",
       "      <td>0.305814</td>\n",
       "      <td>0.294449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.772673</td>\n",
       "      <td>0.786358</td>\n",
       "      <td>0.931587</td>\n",
       "      <td>0.918850</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.090432</td>\n",
       "      <td>0.914642</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.628647</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.743453</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.658207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 702 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     values__variance_larger_than_standard_deviation  \\\n",
       "1.0                                              0.0   \n",
       "2.0                                              0.0   \n",
       "3.0                                              0.0   \n",
       "4.0                                              0.0   \n",
       "\n",
       "     values__has_duplicate_max  values__has_duplicate_min  \\\n",
       "1.0                        0.0                        0.0   \n",
       "2.0                        0.0                        0.0   \n",
       "3.0                        0.0                        0.0   \n",
       "4.0                        0.0                        0.0   \n",
       "\n",
       "     values__has_duplicate  values__sum_values  values__abs_energy  \\\n",
       "1.0                    0.0            0.000000            0.000000   \n",
       "2.0                    0.0            1.000000            0.193383   \n",
       "3.0                    0.0            0.247102            1.000000   \n",
       "4.0                    0.0            0.090432            0.914642   \n",
       "\n",
       "     values__mean_abs_change  values__mean_change  \\\n",
       "1.0                 0.800185             1.000000   \n",
       "2.0                 1.000000             0.473259   \n",
       "3.0                 0.214757             0.513885   \n",
       "4.0                 0.000000             0.000000   \n",
       "\n",
       "     values__mean_second_derivative_central  values__median  ...  \\\n",
       "1.0                                1.000000        0.000000  ...   \n",
       "2.0                                0.613680        0.830239  ...   \n",
       "3.0                                0.221325        1.000000  ...   \n",
       "4.0                                0.000000        0.628647  ...   \n",
       "\n",
       "     values__fourier_entropy__bins_3  values__fourier_entropy__bins_5  \\\n",
       "1.0                         1.000000                         1.000000   \n",
       "2.0                         0.649919                         0.610735   \n",
       "3.0                         0.000000                         0.367919   \n",
       "4.0                         0.000000                         0.000000   \n",
       "\n",
       "     values__fourier_entropy__bins_10  values__fourier_entropy__bins_100  \\\n",
       "1.0                          1.000000                           1.000000   \n",
       "2.0                          0.762944                           0.674596   \n",
       "3.0                          0.305814                           0.294449   \n",
       "4.0                          0.000000                           0.000000   \n",
       "\n",
       "     values__permutation_entropy__dimension_3__tau_1  \\\n",
       "1.0                                         1.000000   \n",
       "2.0                                         0.402455   \n",
       "3.0                                         0.000000   \n",
       "4.0                                         0.743453   \n",
       "\n",
       "     values__permutation_entropy__dimension_4__tau_1  \\\n",
       "1.0                                         0.000000   \n",
       "2.0                                         0.871087   \n",
       "3.0                                         0.772673   \n",
       "4.0                                         1.000000   \n",
       "\n",
       "     values__permutation_entropy__dimension_5__tau_1  \\\n",
       "1.0                                         0.000000   \n",
       "2.0                                         0.763492   \n",
       "3.0                                         0.786358   \n",
       "4.0                                         1.000000   \n",
       "\n",
       "     values__permutation_entropy__dimension_6__tau_1  \\\n",
       "1.0                                         0.000000   \n",
       "2.0                                         0.715084   \n",
       "3.0                                         0.931587   \n",
       "4.0                                         1.000000   \n",
       "\n",
       "     values__permutation_entropy__dimension_7__tau_1  \\\n",
       "1.0                                         0.000000   \n",
       "2.0                                         0.576456   \n",
       "3.0                                         0.918850   \n",
       "4.0                                         1.000000   \n",
       "\n",
       "     values__mean_n_absolute_max__number_of_maxima_7  \n",
       "1.0                                         0.000000  \n",
       "2.0                                         0.150188  \n",
       "3.0                                         1.000000  \n",
       "4.0                                         0.658207  \n",
       "\n",
       "[4 rows x 702 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data normalization,ignore all warning\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "cols = ext_feature_clean.select_dtypes(np.number).columns\n",
    "ext_feature_clean[cols] = minmax_scale(ext_feature_clean[cols])\n",
    "ext_feature_clean.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "df9a5d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    label  values__has_duplicate_max  values__root_mean_square  \\\n",
      "1.0     L                        0.0                  0.000000   \n",
      "2.0     L                        0.0                  0.260626   \n",
      "3.0     R                        0.0                  1.000000   \n",
      "4.0     R                        0.0                  0.798402   \n",
      "\n",
      "     values__last_location_of_maximum  values__first_location_of_maximum  \\\n",
      "1.0                          1.000000                           1.000000   \n",
      "2.0                          0.000000                           0.000000   \n",
      "3.0                          0.919085                           0.924302   \n",
      "4.0                          0.984648                           0.990402   \n",
      "\n",
      "     values__percentage_of_reoccurring_values_to_all_values  \\\n",
      "1.0                                           0.000000        \n",
      "2.0                                           0.176825        \n",
      "3.0                                           0.926766        \n",
      "4.0                                           1.000000        \n",
      "\n",
      "     values__percentage_of_reoccurring_datapoints_to_all_datapoints  \\\n",
      "1.0                                           0.000000                \n",
      "2.0                                           0.179334                \n",
      "3.0                                           0.936975                \n",
      "4.0                                           1.000000                \n",
      "\n",
      "     values__ratio_value_number_to_time_series_length  \\\n",
      "1.0                                          1.000000   \n",
      "2.0                                          0.828537   \n",
      "3.0                                          0.057154   \n",
      "4.0                                          0.000000   \n",
      "\n",
      "     values__symmetry_looking__r_0.25  \\\n",
      "1.0                               0.0   \n",
      "2.0                               0.0   \n",
      "3.0                               0.0   \n",
      "4.0                               0.0   \n",
      "\n",
      "     values__symmetry_looking__r_0.30000000000000004  ...  \\\n",
      "1.0                                              0.0  ...   \n",
      "2.0                                              0.0  ...   \n",
      "3.0                                              0.0  ...   \n",
      "4.0                                              0.0  ...   \n",
      "\n",
      "     values__fft_coefficient__attr_\"angle\"__coeff_37  \\\n",
      "1.0                                         0.803419   \n",
      "2.0                                         0.000000   \n",
      "3.0                                         1.000000   \n",
      "4.0                                         0.040809   \n",
      "\n",
      "     values__fft_coefficient__attr_\"angle\"__coeff_38  \\\n",
      "1.0                                         1.000000   \n",
      "2.0                                         0.777084   \n",
      "3.0                                         0.812882   \n",
      "4.0                                         0.000000   \n",
      "\n",
      "     values__fft_coefficient__attr_\"angle\"__coeff_41  \\\n",
      "1.0                                         0.597343   \n",
      "2.0                                         0.552416   \n",
      "3.0                                         1.000000   \n",
      "4.0                                         0.000000   \n",
      "\n",
      "     values__fft_coefficient__attr_\"angle\"__coeff_48  \\\n",
      "1.0                                         0.717728   \n",
      "2.0                                         1.000000   \n",
      "3.0                                         0.808133   \n",
      "4.0                                         0.000000   \n",
      "\n",
      "     values__fft_coefficient__attr_\"angle\"__coeff_53  \\\n",
      "1.0                                         0.000000   \n",
      "2.0                                         1.000000   \n",
      "3.0                                         0.960055   \n",
      "4.0                                         0.165159   \n",
      "\n",
      "     values__agg_linear_trend__attr_\"rvalue\"__chunk_len_50__f_agg_\"mean\"  \\\n",
      "1.0                                           1.000000                     \n",
      "2.0                                           0.496719                     \n",
      "3.0                                           0.116279                     \n",
      "4.0                                           0.000000                     \n",
      "\n",
      "     values__agg_linear_trend__attr_\"intercept\"__chunk_len_10__f_agg_\"mean\"  \\\n",
      "1.0                                           0.000000                        \n",
      "2.0                                           0.640380                        \n",
      "3.0                                           0.967743                        \n",
      "4.0                                           1.000000                        \n",
      "\n",
      "     values__agg_linear_trend__attr_\"slope\"__chunk_len_10__f_agg_\"max\"  \\\n",
      "1.0                                           1.000000                   \n",
      "2.0                                           0.233638                   \n",
      "3.0                                           0.003068                   \n",
      "4.0                                           0.000000                   \n",
      "\n",
      "     values__agg_linear_trend__attr_\"slope\"__chunk_len_10__f_agg_\"min\"  \\\n",
      "1.0                                           1.000000                   \n",
      "2.0                                           0.365535                   \n",
      "3.0                                           0.034196                   \n",
      "4.0                                           0.000000                   \n",
      "\n",
      "     values__agg_linear_trend__attr_\"slope\"__chunk_len_10__f_agg_\"mean\"  \n",
      "1.0                                           1.000000                   \n",
      "2.0                                           0.323508                   \n",
      "3.0                                           0.020727                   \n",
      "4.0                                           0.000000                   \n",
      "\n",
      "[4 rows x 51 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nn/h99dr9455yg83gkxbsycq7m40000gn/T/ipykernel_50136/2571192670.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  main[\"label\"] = Y\n",
      "/var/folders/nn/h99dr9455yg83gkxbsycq7m40000gn/T/ipykernel_50136/2571192670.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  main[\"label\"] = Y\n",
      "/Applications/anaconda/anaconda3/lib/python3.9/site-packages/sklearn/feature_selection/_univariate_selection.py:358: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  f_statistic = corr_coef_squared / (1 - corr_coef_squared) * deg_of_freedom\n"
     ]
    }
   ],
   "source": [
    "# 3.0 feature selection\n",
    "main = ext_feature_clean\n",
    "Y = [\"L\",\"L\",\"R\",\"R\"]#label\n",
    "main[\"label\"] = Y\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Separate the features (X) and the target variable (y)\n",
    "X = main.drop(columns=\"label\")\n",
    "y = main[\"label\"]\n",
    "\n",
    "# Encode the target variable to numeric values using LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Remove constant or near-constant features\n",
    "filter1 = VarianceThreshold(threshold=0.1)\n",
    "X_filter1 = filter1.fit_transform(X)\n",
    "\n",
    "# Select the top k features using SelectKBest and f_regression\n",
    "filter2 = SelectKBest(score_func=f_regression, k=50)\n",
    "X_filter2 = filter2.fit_transform(X_filter1, y_encoded)\n",
    "\n",
    "# Get the selected features\n",
    "filter_inx = filter2.get_support(indices=True)\n",
    "selected_features = X.columns[filter_inx]\n",
    "\n",
    "# Print the selected feature names\n",
    "ind = ['label'] + list(selected_features)\n",
    "selected_features_df = main[ind]\n",
    "\n",
    "print(selected_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "71ce63ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#final data check \n",
    "#clean_df = selected_features_df.replace([np.inf, -np.inf], np.nan).dropna(axis=1,how = \"any\")\n",
    "check_for_nan = selected_features_df.isnull().sum().sum()\n",
    "print(check_for_nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "9fbb0290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. create classifier-RF, def function, return rf.fit(X, Y)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# Separate the features (X) and the target variable (y)\n",
    "X = selected_features_df.drop(columns=\"label\")\n",
    "y = selected_features_df[\"label\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "97580cfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "rf.fit(X,  y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "a907e46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downsample complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|█████████████████████████| 1/1 [00:00<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "#4. predict for event, call (def data pre process function)\n",
    "\n",
    "#input feature extraction\n",
    "input_features = pd.read_csv(\"first.csv\")\n",
    "input_features[\"id\"]  = \"0\"\n",
    "input_features = input_features.drop(columns = \"Unnamed: 0\")\n",
    "input_features.rename(columns = {'x':'values'}, inplace = True)\n",
    "input_features[\"id\"] = input_features[\"id\"].astype('float')\n",
    "input_features[\"values\"] = input_features[\"values\"].astype('float')\n",
    "\n",
    "downsampled_input1 = downsample_dataframe(input_features, 1000)\n",
    "\n",
    "downsample_process(input_features, downsampled_input1)\n",
    "\n",
    "input_features_df1 = extract_features(downsampled_input1,column_id=\"id\",\n",
    "                                  default_fc_parameters=ComprehensiveFCParameters(),\n",
    "                                  n_jobs=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "91339935",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input fs\n",
    "ind =list(selected_features)\n",
    "input_fs_df1= input_features_df1[ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "4ffa1e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension matched\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_dim(df1, df2):\n",
    "    if df1.shape[1]-df2.shape[1] == 0:\n",
    "        return print(\"dimension matched\")\n",
    "    else:\n",
    "        return print(\"check the dim\")\n",
    "    \n",
    "check_dim(X,input_fs_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "5510a3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L\n"
     ]
    }
   ],
   "source": [
    "#testing prediction\n",
    "# L test\n",
    "a= rf.predict(input_fs_df1)#dataset in live\n",
    "prediction = a[0]\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "77f116c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [304]\u001b[0m, in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_index, test_index \u001b[38;5;129;01min\u001b[39;00m kf\u001b[38;5;241m.\u001b[39msplit(X):\n\u001b[1;32m     25\u001b[0m     X_train, X_test \u001b[38;5;241m=\u001b[39m X[train_index], X[test_index]\n\u001b[0;32m---> 26\u001b[0m     y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_index\u001b[49m\u001b[43m]\u001b[49m, Y[test_index]\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# SVM\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     svm_res \u001b[38;5;241m=\u001b[39m svm\u001b[38;5;241m.\u001b[39mNuSVC(kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# 5.0 Evaluation\n",
    "import re\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "    \n",
    "folds = 4\n",
    "repeats = 10\n",
    "seed = 3888\n",
    "np.random.seed(seed)\n",
    "\n",
    "kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "acc50_svm = []\n",
    "acc50_rf = []\n",
    "X = np.array(X)\n",
    "for _ in range(repeats):\n",
    "    acc_svm = []\n",
    "    acc_rf = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "        # SVM\n",
    "        svm_res = svm.NuSVC(kernel='linear')\n",
    "        svm_res.fit(X_train, y_train)\n",
    "        y_pred_svm = svm_res.predict(X_test)\n",
    "        acc_svm = accuracy_score(y_test, y_pred_svm)\n",
    "        acc_svm.append(acc_svm)\n",
    "        \n",
    "        # RF\n",
    "        rf = RandomForestClassifier()\n",
    "        rf.fit(X, Y)\n",
    "        y_pred_rf = rf.predict(X_test)\n",
    "        acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "        acc_rf.append(acc_rf)\n",
    "        \n",
    "        # \n",
    "\n",
    "    acc50_svm.append(np.mean(acc_svm))\n",
    "    acc50_rf.append(np.mean(acc_rf))\n",
    "\n",
    "avg1 = np.mean(acc50_svm)\n",
    "avg2 = np.mean(acc50_rf)\n",
    "print(avg1)\n",
    "print(avg2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5892b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
